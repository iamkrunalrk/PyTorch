{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograde (Automatic Differenciation) in PyTorch\n",
    "\n",
    "Autograd is a core component of PyTorch that provides automatic differentiation for tensors. It enables you to compute gradients of tensors with respect to other tensors, which is crucial for training neural networks using gradient-based optimization algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Enabling Autograd\n",
    "\n",
    "Autograd is enabled by default in PyTorch. You typically create tensors and perform operations on them, and PyTorch keeps track of these operations to compute gradients when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Creating tensors\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Perform Operation\n",
    "z = x**2 + y**3\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we've created two tensors `x` and `y` with `requires_grad=True`, which tells PyTorch to track operations on these tensors for gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing Gradients\n",
    "\n",
    "To compute gradients, you can call the `backward()` method on a scalar tensor (usually a loss), and PyTorch will compute gradients for all tensors that are part of the computation graph leading to that scalar tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x:  tensor(4.)\n",
      "Gradient of y:  tensor(27.)\n"
     ]
    }
   ],
   "source": [
    "# Compute Gradient\n",
    "z.backward()\n",
    "\n",
    "# Access gradient\n",
    "grad_x = x.grad\n",
    "grad_y = y.grad\n",
    "\n",
    "print(\"Gradient of x: \", grad_x)\n",
    "print(\"Gradient of y: \", grad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `z` is a scalar tensor, so we can call `backward()` on it. Afterward, we can access the gradients of `x` and `y` using the `grad` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Detaching Tensors\n",
    "\n",
    "Sometimes, you may want to compute gradients for some tensors while excluding others. You can detach tensors to prevent them from participating in gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of a: tensor(6.)\n",
      "Gradient of b: None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Perform operations and detach y\n",
    "c = a ** 2 + b.detach() ** 3\n",
    "\n",
    "# Compute gradients\n",
    "c.backward()\n",
    "\n",
    "grad_a = a.grad\n",
    "grad_b = b.grad\n",
    "\n",
    "print(\"Gradient of a:\", grad_a)  # Gradient of x: tensor(4.)\n",
    "print(\"Gradient of b:\", grad_b)  # Gradient of y: None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we detached `y` before using it in an operation, so it doesn't contribute to the gradient computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Autograd in Neural Networks\n",
    "\n",
    "In neural network training, autograd plays a crucial role. Here's how it works in a simple neural network example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create an instance of the network\n",
    "net = Net()\n",
    "\n",
    "# Create input and target vectors\n",
    "input_data = torch.tensor([[1.0]])\n",
    "target_data = torch.tensor([[2.0]])\n",
    "\n",
    "# Define a loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# Forward Pass\n",
    "output = net(input_data)\n",
    "\n",
    "# Compute the loss\n",
    "loss = criterion(output, target_data)\n",
    "\n",
    "# Backpropogation and optimization\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, autograd automatically computes gradients during the forward and backward passes, allowing the optimizer to update the network's weights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Artificial Neural Network\n",
    "\n",
    "Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the structure and function of the human brain. ANNs consist of interconnected nodes, or artificial neurons, organized in layers. These neurons work together to process and learn from data, making ANNs a powerful tool for various tasks, including classification, regression, and even more complex tasks like image and speech recognition.\n",
    "\n",
    "Here's a detailed breakdown of the key concepts in ANNs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Neurons (Artificial Neurons):\n",
    "Neurons are the basic building blocks of ANNs. Each neuron receives one or more inputs, processes them, and produces an output.\n",
    "In mathematical terms, a neuron computes a weighted sum of its inputs, adds a bias term, and applies an activation function to produce the output.\n",
    "\n",
    "The activation function introduces non-linearity into the model, allowing ANNs to learn complex relationships in data.\n",
    "\n",
    "\n",
    "### 2. Layers:\n",
    "Neurons are organized into layers within an ANN. The three primary types of layers are:\n",
    "\n",
    "**Input Layer**: The initial layer that receives the raw input data.\n",
    "\n",
    "**Hidden Layers**: Intermediate layers between the input and output layers. These layers are responsible for feature extraction and representation learning.\n",
    "\n",
    "**Output Layer**: The final layer that produces the model's predictions or outputs.\n",
    "\n",
    "\n",
    "### 3. Weights and Biases:\n",
    "Each connection between neurons in adjacent layers has an associated weight, which represents the strength of that connection.\n",
    "Additionally, each neuron has a bias term, which helps shift the activation function's output.\n",
    "During training, the ANN learns optimal weight and bias values through a process called backpropagation.\n",
    "\n",
    "\n",
    "### 4. Activation Functions:\n",
    "Activation functions introduce non-linearity into the model, allowing it to capture complex patterns in the data.\n",
    "Common activation functions include the sigmoid, ReLU (Rectified Linear Unit), tanh (hyperbolic tangent), and softmax functions.\n",
    "\n",
    "\n",
    "### 5. Feedforward and Backpropagation:\n",
    "In a feedforward pass, data is processed through the network from the input layer to the output layer to produce predictions.\n",
    "Backpropagation is the training process where errors (differences between predictions and actual values) are propagated backward through the network to update weights and biases using optimization algorithms like gradient descent.\n",
    "\n",
    "\n",
    "### 6. Loss Function:\n",
    "The loss function quantifies the difference between the model's predictions and the actual target values.\n",
    "Common loss functions include mean squared error (MSE) for regression tasks and cross-entropy for classification tasks.\n",
    "\n",
    "\n",
    "### 7. Training and Optimization:\n",
    "The goal during training is to minimize the loss function, effectively improving the model's performance.\n",
    "Optimization algorithms like gradient descent are used to update weights and biases iteratively to reach the optimal values.\n",
    "Learning rate, a hyperparameter, determines the step size in the optimization process.\n",
    "\n",
    "\n",
    "### 8. Hyperparameters:\n",
    "Hyperparameters are settings and configurations that are not learned by the model but are set by the user.\n",
    "Examples include the number of hidden layers, the number of neurons in each layer, learning rate, and batch size.\n",
    "\n",
    "\n",
    "### 9. Overfitting and Regularization:\n",
    "Overfitting occurs when a model performs well on training data but poorly on unseen data.\n",
    "Regularization techniques, such as dropout and L1/L2 regularization, help mitigate overfitting by adding constraints to the model.\n",
    "\n",
    "\n",
    "Now, let's see an example of creating a simple feedforward neural network using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2) # Input size 2, Output Size 2\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(2, 1) # Input Size 2, Output Size 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Create an instance of the neural network\n",
    "model = SimpleNN()\n",
    "\n",
    "# Input Data\n",
    "input_data = torch.tensor([1.0, 2.0])\n",
    "\n",
    "# Forward pass to make prediction\n",
    "output = model(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we've defined a simple feedforward neural network using PyTorch's `nn.Module`. The network takes an input of size 2, passes it through two fully connected layers with ReLU activation, and produces a single output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

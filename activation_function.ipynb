{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Activation functions are a fundamental component of neural networks. They introduce non-linearity to the model, enabling neural networks to learn complex relationships and solve a wide range of tasks. In this detailed explanation, We'll cover various activation functions commonly used in neural networks, along with code examples in PyTorch.\n",
    "\n",
    "### Neural Networks Basics: Activation Functions\n",
    "\n",
    "Activation functions are applied to the output of each neuron (or node) in a neural network layer. They determine whether a neuron should \"fire\" and pass its signal to the next layer. Here are some common activation functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sigmoid Activation Function:\n",
    "\n",
    "* Range: (0, 1)\n",
    "* Often used in the output layer for binary classification problems. <br>\n",
    "* Squashes input values to a sigmoid-shaped curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Mathematically, Sigmoid is defined as\n",
    "# f (x) = 1 / (1 + exp(-x))\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Hyperbolic Tangent (Tanh) Activation Function:\n",
    "\n",
    "* Range: (-1, 1)\n",
    "* Similar to the sigmoid but centered around zero.\n",
    "* Often used in hidden layers of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematically, tanh is defined as:\n",
    "# f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "tanh = nn.Tanh()\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Rectified Linear Unit (ReLU) Activation Function:\n",
    "\n",
    "* Range: [0, ∞)\n",
    "* Most widely used activation function.\n",
    "* Simple and computationally efficient.\n",
    "* Introduces non-linearity by thresholding negative values to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematically, ReLU is defined as:\n",
    "# f(x) = max(0, x)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Leaky ReLU Activation Function:\n",
    "\n",
    "* Variation of ReLU.\n",
    "* Allows a small gradient for negative values to prevent dead neurons.\n",
    "* Typically, α (the leaky slope) is set to a small positive value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu = nn.LeakyReLU(0.01) # You can adjust the slope. 0.01 in this case.\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = leaky_relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parametric ReLU (PReLU) Activation Function:\n",
    "\n",
    "* Similar to Leaky ReLU, but the leaky slope is learned during training.\n",
    "* Allows the network to adapt the slope for each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelu = nn.PReLU(num_parameters=1) # One slope parameter per channel\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = prelu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Exponential Linear Unit (ELU) Activation Function:\n",
    "\n",
    "* Range: (-α, ∞) where α is a positive constant.\n",
    "* Similar to ReLU but smooth for negative values.\n",
    "* Prevents dead neurons and can lead to faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "elu = nn.ELU(alpha=1.0) # You can adjust the α parameter\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = elu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Swish Activation Function:\n",
    "\n",
    "* Proposed by Google researchers as an alternative to ReLU.\n",
    "* Combines the advantages of ReLU and Sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = swish(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Softmax\n",
    "\n",
    "* The softmax activation is commonly used in the output layer for multiclass classification problems. \n",
    "* It converts a vector of real numbers into a probability distribution over multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematically Softmax is defined as\n",
    "# softmax(x)_i = exp(x_i) / sum(exp(x_j) for j in range(N))\n",
    "\n",
    "softmax = nn.Softmax(dim=0)\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "output = softmax(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

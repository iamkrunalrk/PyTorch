{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "Loss functions, also known as cost functions or objective functions, play a crucial role in training neural networks. They quantify the error or discrepancy between the predicted outputs of a neural network and the actual target values. In this detailed explanation, We'll cover various loss functions commonly used in neural networks, along with code examples in PyTorch.\n",
    "\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "Loss functions are critical in the training of neural networks. They serve as the guidance for the optimization process, helping the model to learn and adjust its parameters. The choice of loss function depends on the type of task and the nature of the data. Here are some common loss functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE) Loss:\n",
    "\n",
    "* Suitable for regression problems.\n",
    "* Measures the average squared difference between predicted values and target values.\n",
    "* Encourages the model to produce predictions close to the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "predicted = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "target = torch.tensor([0.8, 1.9, 3.2])\n",
    "\n",
    "loss = criterion(predicted, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-Entropy Loss (Binary Cross-Entropy and Categorical Cross-Entropy):\n",
    "\n",
    "* Suitable for classification problems.\n",
    "* Measures the dissimilarity between predicted class probabilities and true class labels.\n",
    "* Binary Cross-Entropy is used for binary classification, while Categorical Cross-Entropy is used for multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Cross-Entropy Loss\n",
    "criterion_binary = nn.BCELoss()\n",
    "predicted = torch.tensor([0.8, 0.2], requires_grad=True)\n",
    "target = torch.tensor([1.0, 0.0])\n",
    "\n",
    "loss_binary = criterion_binary(predicted, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Cross-Entropy Loss\n",
    "criterion_categorical = nn.CrossEntropyLoss()\n",
    "predicted = torch.tensor([[0.2, 0.8],[0.9, 0.1],[0.4, 0.6]], requires_grad=True)\n",
    "target = torch.tensor([1, 0, 1], dtype=torch.long)\n",
    "\n",
    "loss_categorical = criterion_categorical(predicted, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hinge Loss:\n",
    "\n",
    "* Suitable for support vector machines (SVMs) and binary classification.\n",
    "* Encourages correct classification by maximizing the margin between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.HingeEmbeddingLoss()\n",
    "predicted = torch.tensor([0.9, -0.5], requires_grad=True)\n",
    "target = torch.tensor([1.0, -1.0])\n",
    "\n",
    "loss = criterion(predicted, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Huber Loss:\n",
    "\n",
    "* A combination of L1 and L2 losses.\n",
    "* Less sensitive to outliers compared to MSE.\n",
    "* Suitable for regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.HuberLoss()\n",
    "predicted = torch.tensor([2.0, 6.0], requires_grad=True)\n",
    "target = torch.tensor([1.0, 5.0])\n",
    "\n",
    "loss = criterion(predicted, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Kullback-Leibler Divergence (KL Divergence):\n",
    "* Used in probabilistic models and variational autoencoders.\n",
    "* Measures the difference between two probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "p = torch.tensor([0.25, 0.75])\n",
    "q = torch.tensor([0.3, 0.7])\n",
    "\n",
    "loss = F.kl_div(torch.log(q), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Custom Loss Function:\n",
    "\n",
    "* You can define custom loss functions tailored to specific tasks.\n",
    "* This is often necessary for tasks with unique requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(predicted, target):\n",
    "    error  = torch.abs(predicted - target)\n",
    "    return torch.mean(error)\n",
    "\n",
    "predicted = torch.tensor([2.0, 6.0], requires_grad=True)\n",
    "target = torch.tensor([1.5, 5.0])\n",
    "\n",
    "loss = custom_loss(predicted, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
